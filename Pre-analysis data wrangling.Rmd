---
title: "Pre-analyses data wrangling"
author: "Vania Rolon"
date: "2023-01-10"
output:
  html_document: default
  pdf_document: default
---

The original raw data are saved on a password-protected .xlsx document. Data were cleaned on SPSS v.26 and a new .sav file called *DRad cleaned data* was created. To work with this file on R, data need to be imported using the haven package (or another package that can import .sav files). If the haven package has not been installed in R before, then the command install.packages("haven") must be run. Following this, the command library(haven) must be run every time R is opened or every time a new session is ran.

Note that comments in chunks of code are done using the # sign. Lines with these at the beginning will not run. In the code below, the install.packages command will not run. If this command is needed, delete the # and run the line. It is recommended to add the # again after installing the package the first time so as not to keep installing and reinstalling the package in future sessions. The library() command, on the other hand, is needed every time R is reopened.

```{r}
#install.packages("haven")
library(haven)

#import .sav data and assigning it to the object DRad. The path will vary from user to user, as everyone will have saved the cleaned data in different locations of their devices. Replace this path accordingly
drad <- read_sav("C:/Users/Vania/Desktop/DRad/data and code/SPSS/DRad cleaned data.sav")
nrow(drad)
```

# Missing data

When it comes to items on scales, all participants should have completed all items. This seems to be the case for most participants, but the data set contains some dropout cases where participants seemed to have dropped out early on and consequently have missing data for most of the survey items (i.e., if Qualtrics was meant to deliver 1,000 participants, but some were dropped either because they decided not to participate anymore or some other reason, then Qualtrics likely would have found someone to take that participant's place, but that participant's response was still recorded and sent to us).

These dropout responses would be irrelevant for analyses and need to be removed because most items are missing. However there is still a second kind of incomplete response; those where participants did complete most items but skipped a few. When aggregating items to get the final constructs, SPSS syntax was set to consider cases listwise. That is, if construct 1 was to be an aggregate of items 1 to 4, and a participant only completed the first 3 items, then their score on the final construct would be NA.

As a result, it is possible to add the number of NAs on all final construct (e.g., SDO, collective self-esteem, etc.). Greater number of total NAs likely mean that the response in question is a dropout response. Smaller numbers of total NAs would suggest these are incomplete responses, of which we should not have many considering the arrangement with Qualtrics whereby they were supposed to provide complete responses. The code below sums the NAs on all columns of interest for the final model.

```{r}
library(tidyverse)
drad$countNA <- rowSums(is.na(drad[, c(which(colnames(drad) == "SDO_D"):which(colnames(drad) == "Alienation"), 
                                       which(colnames(drad) == "PolAt_1"), which(colnames(drad) == "PolAt_2"),
                                       which(colnames(drad) == "Deprivation_1"), which(colnames(drad) == "Deprivation_2"),
                                       which(colnames(drad) == "UkrMigrant"), which(colnames(drad) == "RusMigrant"))]))

table(drad$countNA) #111 cases where all 26 columns examined had NAs
sum(is.na(drad$Nationality) & drad$countNA == 26) #92 cases where nationality AND all 26 columns are missing, need to make table by countNA and Nationality. The 92 cases where nationality and the columns are missing are dropouts

NAtable <- table(drad$countNA, as.factor(drad$Nationality), useNA = "always")
colnames(NAtable) <- c("Austrian", "Bosnian", "Finnish", "French", "German", "Hungarian", "Israeli", "Italian", "Jordanian", "Kosovan", "Polish", "Serbian", "Slovenian", "Turkish", "Georgian", "English", "Other", "NAs")
names(dimnames(NAtable)) <- c("Total NAs", "Nationality")
NAtable

write.table(NAtable, file = "table of NAs in final constructs by country.txt", sep = ",", quote = FALSE, row.names = TRUE)
```

## "Other" Nationality

There were several cases under "Other" for Nationality. Georgian participants, unlike other samples, were not collected by Qualtrics, so the attrition rate for them was higher. However, partners also mentioned over recruiting participants and said they had checked for completion so that we had roughly n = 200 completed responses. The code below was written to check whether these "Other" responses are from the Georgian partners.

```{r}
#Repeat NA table but using Residence

NAtable_res <- table(drad$countNA, as.factor(drad$Residence), useNA = "always")
colnames(NAtable_res) <- c("Austria", "Bosnia", "Finland", "France", "Georgia", "Germany", "Hungary", "Israel", "Italy", "Jordan", "Kosovo", "Poland", "Serbia", "Slovenia", "Turkey", "UK", "NAs")
names(dimnames(NAtable_res)) <- c("Total NAs", "Residence")
NAtable_res

write.table(NAtable_res, file = "table of NAs in final constructs by residence.txt", sep = ",", quote = FALSE, row.names = TRUE)
```

The "Other" participants seem to be spread out across countries when looking at the data by Residence. We do get 140 full responses for Georgia, and 51 where only one construct is missing. This does give close to 200 completed responses.

Because there was no exclusion criteria that said participants had to be nationals, further analyses use Residence rather than Nationality. Responses were one or two constructs are missing were also kept, as they are still usefu for other exploratory and non-SEM analyses. Two datasets were thus created: a data set with 100% complete responses only, and one with mostly complete responses, defined as those were 2 or fewer constructs were missing (i.e., countNA =< 2)

## Creating new data set with only complete responses, and one with partially complete (countNA =< 2)

```{r}
drad_complete <- drad %>%
  filter(countNA == 0)
nrow(drad_complete) #this does track with the number of complete responses in both the Nationality and Residence tables

drad_partial <- drad %>%
  filter(countNA <= 2)
nrow(drad_partial)

library(readr)
write_csv(drad_complete, "C:/Users/Vania/Desktop/DRad/data and code/DRad complete responses for SEM.csv") #keeps less detail than .sav
write_sav(drad_complete,"C:/Users/Vania/Desktop/DRad/data and code/SPSS/complete responses for SEM.sav")

write_sav(drad_partial,"C:/Users/Vania/Desktop/DRad/data and code/SPSS/partially complete responses for SEM.sav")
```

# Download complete and partially complete datasets for future R sessions

```{r}
#library(haven) #if the haven has not yet been downloaded for the new session
drad_complete <- read_sav("C:/Users/Vania/Desktop/DRad/data and code/SPSS/complete responses for SEM.sav")
drad_partial <- read_sav("C:/Users/Vania/Desktop/DRad/data and code/SPSS/partially complete responses for SEM.sav")
```

# Comparing original data when recoding into same variables vs different variables (21/01/23)

When starting to run the pre-registered model, the measurement model had poor fit. A look at CFAs for SDO and collective self-esteem showed poor loadings for some items. 

Further moving to compute Cronbach's alphas for all scales resulted in an even greater puzzle: despite recoding the necessary items for all our scales on SPSS, R would give the warning "Warning: Some items were negatively correlated with the total scale and probably should be reversed." R would also pinpoint which items should be reversed. For example, when using the whole dataset, R stated that items 1, 3, 6, 7, 9, and 11 of the Authoritarianism-Conservatism scale were negatively correlated with the total scale and should be reversed. These items match the items that need to be reverse scored on the original scale. Thus, at first glance one would think SPSS did not reverse code items properly. However, all 1-7 scales that had reverse items were coded in the same chunk. These were items in the SDO, collective self-esteem, realistic and symbolic threat, anomie, authoritarianism-conservatism, and political trust scales. If SPSS did not recode the authoritarianism-conservatism items properly, the same should have been the case for items in the other scales, but R did not find negative correlations for these. 

Stranger still, when using only the UK subset, which was extracted from the complete responses dataset, and computing alphas, R now mentioned that items 1, 3, 6, 7, 9, 11 of the authoritarianism-conservatism scale, items 3, 5, and 7 of symbolic threat, and items 1, 2, 3, 4, 5, 10, and 12 should be reversed. These items once again coincided with the items that need to be recoded (except for anomie where the items that R recommends recoding are the ones that shoult NOT be recoded according to the anomie scale).

To figure if perhaps there were coding issues when cleaning data on SPSS and recoding variables into the same column, a new syntax file was created and the raw data were cleaned a second time but this time recoding variables into different columns. If the first syntax recoding variables into the same column did not have mistakes, then the final aggregated constructs in both cleaned datasets (and before removing incomplete responses) should be identical. We can compare these columns with R in the code below:

```{r}
library(haven)
library(tidyverse)

#import .sav data files
drad_samecolumns <- read_sav("C:/Users/Vania/Desktop/DRad/data and code/SPSS/DRad cleaned data.sav")
drad_diffcolumns <- read_sav("C:/Users/Vania/Desktop/DRad/data and code/SPSS/DRad cleaned data, recodes saved into different variables.sav")
nrow(drad_samecolumns)
nrow(drad_diffcolumns)

identical(drad_diffcolumns$SDO_D, drad_samecolumns$SDO_D)
identical(drad_diffcolumns$SDO_E, drad_samecolumns$SDO_E)
identical(drad_diffcolumns$SDO_T, drad_samecolumns$SDO_T)
identical(drad_diffcolumns$SEsteem, drad_samecolumns$SEsteem)
identical(drad_diffcolumns$Narcissism, drad_samecolumns$Narcissism)
identical(drad_diffcolumns$RThreat, drad_samecolumns$RThreat)
identical(drad_diffcolumns$SThreat, drad_samecolumns$SThreat)
identical(drad_diffcolumns$Populism, drad_samecolumns$Populism)
identical(drad_diffcolumns$Extremism, drad_samecolumns$Extremism)
identical(drad_diffcolumns$Anomie_sfabric, drad_samecolumns$Anomie_sfabric)
identical(drad_diffcolumns$Anomie_leader, drad_samecolumns$Anomie_leader)
identical(drad_diffcolumns$Anomie, drad_samecolumns$Anomie)
identical(drad_diffcolumns$OnlineGroupID, drad_samecolumns$OnlineGroupID)
identical(drad_diffcolumns$Authoritarianism, drad_samecolumns$Authoritarianism)
identical(drad_diffcolumns$Conservatism, drad_samecolumns$Conservatism)
identical(drad_diffcolumns$Authoritarianism_Conservatism, drad_samecolumns$Authoritarianism_Conservatism)
identical(drad_diffcolumns$PolTrust, drad_samecolumns$PolTrust)
identical(drad_diffcolumns$Cohesion, drad_samecolumns$Cohesion)
identical(drad_diffcolumns$RusAtt, drad_samecolumns$RusAtt)
identical(drad_diffcolumns$Alienation, drad_samecolumns$Alienation)
```

We see that the final constructs are indeed identical across datasets, whether we use the set that recoded variables into the same columns, or the set that recoded variables into different columns. As such, we can move on using the more practical one: the one that recoded things into the same column, as this frees us from having to constantly remember to include _R items when building models that use single items (i.e, mainly SEMs). 

This also means that, unless there was an error with the code when computing the alphas, the puzzling flaggings we saw are accurate, and the problem is not with the coding done to clean and analyse the data, but with the data itself. The final comparison to be done is to get the alphas with SPSS and see if it gives us the same warning that R did regarding negatively correlated items.



